Operational logic: casting implements a configurable CastConfig-backed engine that infers column types through pattern matching, numeric/boolean/json heuristics, semantic object detection, and optional parallel processing, then converts Series safely while validating null growth, splitting large inputs into chunked runs to limit memory pressure.
Real-world problem addressed: it provides configurable DataFrame casting with inference, validation, logging snapshots, and presets (auto_cast, fast_cast, safe_cast) so heterogeneous inputs can be coerced without ad-hoc casting code.
Control flow: cast_df converts inputs via _to_dataframe, logs small inputs, exits early on empty frames, and chooses chunked processing through _cast_df_chunked when row count exceeds chunk_size; otherwise it uses _transform_dtypes_enhanced or per-column _process_column (with dtype overrides, sampling-based inference, and optional ThreadPool when parallel and enough columns), _convert_series_safe handles guarded conversions, and return_dtype_meta triggers semantic object detection and log_json of dtype transitions.
Failure modes: invalid input types raise ValueError; dtype overrides or conversions that fail are caught and fall back to the original series (conversion errors only logged at debug or via best-effort log_json); conversions rejected due to null-increase checks warn and leave data unchanged; logging helpers are wrapped to avoid raising; the chunked path recurses into cast_df when use_transform is False, repeating logging/metadata per chunk.
