Operational logic: call_crud orchestrates execution by validating table metadata, column names, and constraints before invoking crud_v3 lazy SQL generation, then chunks the normalized data, attempts bulk execution via executemany, and falls back to per-row retries while capturing chunk stats and aggregating successes/failures.
Real-world problem addressed: it wraps crud_v3 SQL generators with SQLAlchemy reflection and chunked execution so workflows can insert/upsert/update/select via engines without hand-writing dialect SQL, while surfacing row-level failures.
Control flow: table metadata is cached via lru_cache, column names are validated case-insensitively, constraints are checked against reflected PK/unique sets (skipped if inspection fails), `_execute_chunk_bulk` attempts executemany when all SQL strings match, `_execute_lazy_fallback` runs per-row with error capture, and `execute_crud_operation` optionally aligns data via schema_align, normalizes rows, chunks input, builds validated SQL, and executes each chunk with bulk-then-lazy flow (update uses only the first row); generate_select_sql normalizes WHERE/column names before delegating to lazy_select_sql.
Failure modes: missing tables/columns/constraints raise ValueError; if inspector fails, uniqueness validation is bypassed instead of enforced; schema_align ImportError only logs a warning; bulk execution failures fall back to row-wise execution with errors collected (first 5 stored); chunk-level exceptions mark the chunk fully failed but do not abort later chunks; empty datasets return an "empty" stats object; operations run within a single transaction where chunk-level exceptions are suppressed, allowing all successful chunks to be committed while failed chunks are logged.
