Operational logic: data_profiler offers profiling, sampling, and key-detection utilities; profile_dataframe computes type heuristics, uniqueness, and null stats per column, sample_dispatcher filters/sorts and returns balanced head/tail slices with metadata, and get_pk uses profile metadata plus composite-building to infer a unique or fallback primary key while logging decisions.
Real-world problem addressed: it supports ETL diagnostics by summarizing DataFrame health, generating representative samples after optional null filtering, and producing deterministic keys so downstream upserts can reference stable identifiers without bespoke profiling code.
Control flow: profile_dataframe iterates columns, infers datetime candidacy, and logs high-null columns; sample_dispatcher validates inputs, optionally drops null rows, sorts by a key or index, slices balanced ends, logs warnings/metadata, and records sampling stats; get_pk validates dfinfo, identifies unique columns or builds composites incrementally (optionally adding a datetime column, iterating candidates, and falling back to non-unique composites) while logging metadata and JSON snapshots.
Failure modes: profile_dataframe returns an empty frame when input is empty; _validate_inputs raises ValueError for empty df/dfinfo or missing dfinfo columns; sample_dispatcher raises for invalid percent ranges and returns empty samples if null filtering removes all rows; get_pk raises when uniqueness_threshold is out of [0,1] or dfinfo is invalid, can exhaust max_composite_size leaving a non-unique composite with warnings; logging/json helpers swallow serialization errors.
